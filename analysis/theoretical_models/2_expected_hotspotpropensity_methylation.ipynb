{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute theoretical expected no. hotspots for SBS1\n",
    "\n",
    "## * SBS1 only CpG sites\n",
    "\n",
    "## * modified profiles with custom channels accounting for genome-wide CpG methylation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from decimal import *\n",
    "import os\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "from functools import reduce\n",
    "from operator import concat\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import triplets, mut_key_gen, mut_key_gen_cosmic, sum_dict, sbs_normalize, sbs_format, sbs_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunksizes = ['1000kb'] # Alternatively use: ['1000kb', '500kb', '250kb', '100kb', '50kb', '25kb', '10kb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cpg_data_folder = f'{main_dir}/methylation/data/fractional_methylation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins_dir = f'{main_dir}/genomic_bins/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "triplets = ['ACG', 'CCG', 'GCG', 'TCG']\n",
    "triplets_extended = reduce(concat, [[t, t + '_meth'] for t in triplets], [])\n",
    "colors = ['pink', 'brown', 'green']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load COSMIC Signatures and sort channels canonically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosmic_df = pd.read_csv('./COSMIC_v3.2_SBS_GRCh38.txt', sep='\\t', index_col='Type')\n",
    "canonical_context_sorting = list(mut_key_gen_cosmic())\n",
    "cosmic_df = cosmic_df.loc[canonical_context_sorting]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load CpG annotated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: some bins may be missing from the input data due to lack of CpG methylation data overlapping it. To fix this, we add missing bins with 0 values in methylation categories. We then add their CpG trinucleotide counts as missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mtypes = [\n",
    "    'ME_POS_MUT_POS', \n",
    "    'ME_POS_MUT_NEG', \n",
    "    'ME_NEG_MUT_POS',\n",
    "    'ME_NEG_MUT_NEG' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000kb\n",
      "CPU times: user 1.96 s, sys: 105 ms, total: 2.06 s\n",
      "Wall time: 5.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Merge datasets per cancer-type and bin size\n",
    "\n",
    "data_dict = defaultdict(dict)\n",
    "for binsize in chunksizes: \n",
    "    \n",
    "    # Laod mappable bins\n",
    "    bins_f = f'{bins_dir}/hg38_{binsize}_bin.nodrivers.filtered.mappable_positions.autosomes.binids.txt'\n",
    "    bins_total = pd.read_csv(bins_f, sep='\\t', header=0)['BINID'].tolist()\n",
    "    \n",
    "    ### Load trinucleotide composition per bin\n",
    "    trinuc_comp_f = f'{main_dir}/genomic_bins/data/hg38_{binsize}_bin.nodrivers.filtered.mappable_positions.autosomes.trinuc_per_bin.json'\n",
    "    with open(trinuc_comp_f, 'rt') as fd: \n",
    "        trinuc_dict = json.load(fd)\n",
    "    \n",
    "    print(binsize)\n",
    "    for ctype in ['COADREAD', 'ESOPHA_STOMACH', 'NSCLC']:\n",
    "        # List files to merge\n",
    "        fns = []\n",
    "        for fn in glob.glob(os.path.join(cpg_data_folder, f'SBS1_{ctype}*.{binsize}.*.counts_methyl_muts.tsv')):\n",
    "            fns.append(os.path.basename(fn))\n",
    "        # Merge files\n",
    "        lines = []\n",
    "        for file in fns: \n",
    "            fn = os.path.join(cpg_data_folder, file)\n",
    "            df = pd.read_csv(fn, sep='\\t', header=0)\n",
    "            lines.append(df)\n",
    "        df_with_data = pd.concat(lines)\n",
    "\n",
    "        # Add missing bins\n",
    "        epigenomes = df['EPIGENOMES'].iloc[0]\n",
    "        for binid in set(bins_total).difference(set(df_with_data['BINID'].unique())): \n",
    "            chrom = binid.split(':')[0]\n",
    "            for trinuc in triplets:\n",
    "                # Leave methylation data with 0 counts\n",
    "                for mtype in mtypes: \n",
    "                    lines += [pd.DataFrame([['SBS1', epigenomes, 0.5, chrom, binid, trinuc, mtype, 0]], columns=df.columns)]\n",
    "                # Add trinucleotide counts of the bin to \"MISSING\"\n",
    "                trinuc_counts_bin = trinuc_dict[binid][trinuc]\n",
    "                lines += [pd.DataFrame([['SBS1', epigenomes, 0.5, chrom, binid, trinuc, 'MISSING', trinuc_counts_bin]], columns=df.columns)]\n",
    "            \n",
    "        data_dict[binsize][ctype] = pd.concat(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methylatated vs non-methylated mutation rate analysis\n",
    "\n",
    "The goal is 1) come up with a mutational profile that takes into account the mutability differences between methylated and unmethylated CpGs and 2) to impute the missing CpG values per chunk that are not annotated.\n",
    "\n",
    "In what follows we measure the effect of methylated/non-methylated grouping by each cancer type and triplet context across bins.\n",
    "\n",
    "Imputation of the CpG positions annotated as MISSING is based on the local proportion of methylated/unmethylated CpGs per bin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute mutation fold change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fold_change(x, pcount=1):\n",
    "    y = x.values\n",
    "    return ((y[0] + pcount) / (y[0] + y[1] + pcount)) / ((y[2] + pcount) / (y[2] + y[3] + pcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 293 ms, sys: 8.05 ms, total: 301 ms\n",
      "Wall time: 301 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "fold_change_dict = defaultdict(dict)\n",
    "\n",
    "for binsize in chunksizes: \n",
    "    \n",
    "    for i, (cancer_type, df) in enumerate(data_dict[binsize].items()):\n",
    "        \n",
    "        pcount = int(binsize.split('kb')[0])/1000\n",
    "        fold_change_trinuc = {}\n",
    "        \n",
    "        for triplet in triplets: \n",
    "            tri_df = df.loc[df['TRINUC'] == triplet]\n",
    "            me_pos_mut_pos = tri_df.loc[tri_df['TYPE'] == 'ME_POS_MUT_POS']['COUNT'].sum()\n",
    "            me_pos_mut_neg = tri_df.loc[tri_df['TYPE'] == 'ME_POS_MUT_NEG']['COUNT'].sum()\n",
    "            me_neg_mut_pos = tri_df.loc[tri_df['TYPE'] == 'ME_NEG_MUT_POS']['COUNT'].sum()\n",
    "            me_neg_mut_neg = tri_df.loc[tri_df['TYPE'] == 'ME_NEG_MUT_NEG']['COUNT'].sum()\n",
    "\n",
    "            fc = ((me_pos_mut_pos + pcount) /(me_pos_mut_pos + me_pos_mut_neg + pcount)) / ((me_neg_mut_pos + pcount) /(me_neg_mut_pos + me_neg_mut_neg + pcount))\n",
    "            \n",
    "            fold_change_trinuc[triplet] = fc\n",
    "        \n",
    "        fold_change_dict[binsize][cancer_type] = fold_change_trinuc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create total mutrate dict merging mutation rates across NpCpGs in a bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_total_mutations(x):\n",
    "    y = x.values\n",
    "    return y[0] + y[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000kb\n",
      "CPU times: user 5.56 s, sys: 0 ns, total: 5.56 s\n",
      "Wall time: 5.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a multiindex with BINID, TRINUC\n",
    "# apply get_total_mutations per TRINUC and BINID\n",
    "\n",
    "mutrate_dict = defaultdict(dict)\n",
    "\n",
    "for binsize in chunksizes: \n",
    "    # Pseudocount is relative to binsize\n",
    "    pseudocount = int(binsize.split('kb')[0])/1000\n",
    "\n",
    "    for i, (cancer_type, df) in enumerate(data_dict[binsize].items()):\n",
    "\n",
    "        dg = df.groupby(['BINID', 'TRINUC'])['COUNT'].apply(get_total_mutations).reset_index().rename(columns={'COUNT': 'TOTAL_MUTS'})\n",
    "        dh = dg.groupby(['BINID'])['TOTAL_MUTS'].apply(lambda x: sum(list(x))).reset_index()\n",
    "        mutrate_dict[binsize][(cancer_type, 'SBS1')] = dict(zip(dh['BINID'].values, dh['TOTAL_MUTS']))\n",
    "\n",
    "    for (cancer_type, signature), d in mutrate_dict[binsize].items():\n",
    "        new_d = {k: (v + pseudocount) for k, v in d.items()}\n",
    "        total = sum(new_d.values())\n",
    "        norm_new_d = {k: v / total for k, v in new_d.items()}\n",
    "        mutrate_dict[binsize][(cancer_type, signature)] = norm_new_d\n",
    "    \n",
    "    print(binsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get bin NpCpG composition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute MISSING counts as being in the same meth/non-meth proportion as the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute \n",
    "\n",
    "<span style=\"color:red\">(This code only needs to run once for a given chunksize)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def methylated_composition(x):\n",
    "    \"\"\"Reformat missing counts as methylated or no methylated according to proportion of methylated trinucleotides\"\"\"\n",
    "    \n",
    "    y = x.values\n",
    "    proportion_meth = ((y[0] + y[1] + 1) / (y[0] + y[1] + y[2] + y[3] + 1))\n",
    "    meth = y[0] + y[1] + int(proportion_meth * y[4])\n",
    "    nometh = y[2] + y[3] + int((1 - proportion_meth) * y[4])\n",
    "    return int(meth), int(nometh)\n",
    "\n",
    "def reformat_content_dict(d):\n",
    "    \n",
    "    d_meth = {t + '_meth': v[0] for t, v in d.items()}\n",
    "    d_nometh = {t: v[1] for t, v in d.items()}\n",
    "    return {**d_meth, **d_nometh}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000kb\n",
      "CPU times: user 5.8 s, sys: 0 ns, total: 5.8 s\n",
      "Wall time: 5.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "triplet_content_per_chunk = {}    # we don't write a defaultdict because this is exported\n",
    "\n",
    "for binsize in chunksizes: \n",
    "    triplet_content_per_chunk_bin = {}\n",
    "    for i, (cancer_type, df) in enumerate(data_dict[binsize].items()):\n",
    "\n",
    "        dg = df.groupby(by=['BINID', 'TRINUC'])['COUNT'].apply(methylated_composition).reset_index().rename(columns={'COUNT': 'METH_NOMETH'})\n",
    "        dh = dg.groupby(by=['BINID'])[['TRINUC', 'METH_NOMETH']].apply(lambda r: dict(zip(r['TRINUC'], r['METH_NOMETH']))).reset_index().rename(columns={0: 'CONTENT'})\n",
    "\n",
    "        # triplet_content_per_chunk\n",
    "        # dict ttype -> chunk -> triplet -> count\n",
    "\n",
    "        d = dict(zip(dh['BINID'], dh['CONTENT']))\n",
    "        for chunk, content_dict in d.items():\n",
    "            d[chunk] = reformat_content_dict(content_dict)\n",
    "\n",
    "        triplet_content_per_chunk_bin[cancer_type] = d\n",
    "    triplet_content_per_chunk[binsize] = triplet_content_per_chunk_bin\n",
    "    print(binsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with gzip.open('./cpg_triplet_count.pickle.gz', 'wb') as f:\n",
    "    pickle.dump(triplet_content_per_chunk, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatively, load counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with gzip.open('./cpg_triplet_count.pickle.gz', 'rb') as f:\n",
    "    triplet_content_per_chunk = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "triplet_counts_per_chunk = triplet_content_per_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet counts genomewide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45 ms, sys: 0 ns, total: 45 ms\n",
      "Wall time: 44.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "triplet_content_genomewide = defaultdict(dict)\n",
    "for binsize in chunksizes: \n",
    "    for i, (cancer_type, df) in enumerate(data_dict[binsize].items()):\n",
    "        dg = df.groupby(by=['TRINUC'])['COUNT'].apply(sum).reset_index().rename(columns={'COUNT': 'TRIPLET_CONTENT'})\n",
    "        triplet_content_genomewide[binsize][cancer_type] = dict(zip(dg['TRINUC'], dg['TRIPLET_CONTENT']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C>T mutability per triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000kb\n",
      "CPU times: user 1.62 ms, sys: 0 ns, total: 1.62 ms\n",
      "Wall time: 1.56 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mutability_dict = defaultdict(dict)\n",
    "\n",
    "sig = cosmic_df['SBS1']\n",
    "for binsize in chunksizes: \n",
    "    for cancer_type, d in triplet_content_genomewide[binsize].items():\n",
    "        mutability_dict[binsize][cancer_type] = {}\n",
    "        for t, content in d.items():\n",
    "            ind = sbs_index(t, 'T')\n",
    "            mutability_dict[binsize][cancer_type][t] = sig[ind] / content\n",
    "            mutability_dict[binsize][cancer_type][f'{t}_meth'] = (sig[ind] / content) * fold_change_dict[binsize][cancer_type][t]\n",
    "\n",
    "    for cancer_type, d in mutability_dict[binsize].items():\n",
    "        total = sum(d.values())\n",
    "        new_d = {k: v / total for k, v in d.items()}\n",
    "        mutability_dict[binsize][cancer_type] = new_d\n",
    "    print(binsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TripletRegion class\n",
    "\n",
    "This class implements the method to compute expected number of hotspots described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TripletRegion:\n",
    "    \n",
    "    getcontext().prec = 100  # using instances of decimal.Decimal() we can do arithmetic at an arbitrarily precision level\n",
    "    \n",
    "    def __init__(self, N, L, pa, pb, pc):\n",
    "        \n",
    "        \"\"\"\n",
    "        N: number of sample\n",
    "        L: region length\n",
    "        pa, pb, pc: per-base probabilities of either of 3 possible mutation types\n",
    "        \"\"\"\n",
    "        \n",
    "        self.N = Decimal(N)\n",
    "        self.L = L\n",
    "        self.pa = Decimal(pa)\n",
    "        self.pb = Decimal(pb)\n",
    "        self.pc = Decimal(pc)\n",
    "        self.p_mutation = self.pa + self.pb + self.pc\n",
    "        self.q = 1 - self.p_mutation\n",
    "    \n",
    "    @property\n",
    "    def p_hotspot(self):\n",
    "        t0 = self.q ** self.N\n",
    "        t1 = Decimal(self.N) * self.q ** (self.N - 1) * (self.pa + self.pb + self.pc)\n",
    "        t2 = Decimal(self.N) * Decimal(self.N - 1) * self.q ** (self.N - 2) * (self.pa * self.pb + self.pa * self.pc + self.pb * self.pc)\n",
    "        t3 = Decimal(self.N) * Decimal(self.N - 1) * Decimal(self.N - 2) * self.q ** (self.N - 3) * self.pa * self.pb * self.pc\n",
    "        return float(Decimal(1) - (t0 + t1 + t2 + t3))\n",
    "    \n",
    "    @property\n",
    "    def expected_hotspots(self):\n",
    "        return self.p_hotspot * self.L\n",
    "    \n",
    "    @property\n",
    "    def binomial_distribution(self):\n",
    "        rv = binom(self.L, self.p_hotspot)\n",
    "        return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TripletRegionSingleMutation:\n",
    "    \n",
    "    getcontext().prec = 1000  # using instances of decimal.Decimal() we can do arithmetic at an arbitrarily precision level\n",
    "    \n",
    "    def __init__(self, N, L, pa):\n",
    "        \n",
    "        \"\"\"\n",
    "        N: number of sample\n",
    "        L: region length\n",
    "        pa: per-base probabilities of the mutation type\n",
    "        \"\"\"\n",
    "        \n",
    "        self.N = Decimal(N)\n",
    "        self.L = L\n",
    "        self.pa = Decimal(pa)\n",
    "        self.p_mutation = self.pa\n",
    "        self.q = 1 - self.p_mutation\n",
    "    \n",
    "    @property\n",
    "    def p_hotspot(self):\n",
    "        t0 = self.q ** self.N\n",
    "        t1 = Decimal(self.N) * self.q ** (self.N - 1) * self.pa\n",
    "        return float(Decimal(1) - (t0 + t1))\n",
    "    \n",
    "    @property\n",
    "    def expected_hotspots(self):\n",
    "        return self.p_hotspot * self.L\n",
    "    \n",
    "    @property\n",
    "    def binomial_distribution(self):\n",
    "        rv = binom(self.L, self.p_hotspot)\n",
    "        return rv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected number of hotspots per chunk with variable mutation rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expected_hotspots_per_chunk(ttype, muts_per_sample, N, binsize):\n",
    "    \n",
    "    \"\"\"\n",
    "    ttype: cancer type\n",
    "    muts_per_sample: # mutations per sample \n",
    "    N: # samples\n",
    "    \"\"\"\n",
    "    \n",
    "    # get relative mutation rates per chunk\n",
    "    mutability = mutrate_dict[binsize][(ttype, signature)]\n",
    "    \n",
    "    # dictionary with hotspot rate functions for each chunk\n",
    "    res = {}\n",
    "    \n",
    "    for chunk, abundance_dict in triplet_counts_per_chunk[binsize][ttype].items():\n",
    "                \n",
    "        chunk_mutrate = muts_per_sample * mutrate_dict[binsize][(ttype, 'SBS1')][chunk]\n",
    "        \n",
    "        # estimate relative mutation load for each triplet genome\n",
    "        triplet_region_weights = {t: {'length': triplet_counts_per_chunk[binsize][ttype][chunk][t], 'weights': []} for t in triplets_extended}\n",
    "\n",
    "        for t in triplets_extended:\n",
    "            \n",
    "            triplet_region_weights[t]['weights'] += [mutability_dict[binsize][ttype][t]]\n",
    "            \n",
    "        # mutrate\n",
    "        # dict: triplet -> mutation burden that corresponds to the \"triplet\" region in the chunk per sample, \n",
    "        # given i) the relative mutation rate of the chunk and ii) the number of mutations per sample\n",
    "\n",
    "        load = {t: sum(v['weights']) * v['length'] for t, v in triplet_region_weights.items()}\n",
    "        total_load = sum([v for t, v in load.items()])\n",
    "        if total_load > 0: \n",
    "            mutrate = {t: (v / total_load) * chunk_mutrate for t, v in load.items()}\n",
    "                \n",
    "        # probabilities of for each mutation type within each triplet region\n",
    "        probabilities = {}\n",
    "        for t, v in triplet_region_weights.items():\n",
    "            if v['length'] > 0:\n",
    "                perposition_mutrate = mutrate[t] / v['length']\n",
    "                total_rate = sum(v['weights'])\n",
    "                pa = (v['weights'][0] / total_rate) * perposition_mutrate\n",
    "                probabilities[t] = (pa, )\n",
    "            else: \n",
    "                probabilities[t] = (0, )\n",
    "                    \n",
    "        res[chunk] = 0\n",
    "        region = {}\n",
    "        for t, v in triplet_region_weights.items():\n",
    "                        \n",
    "            region[t] = TripletRegionSingleMutation(N, v['length'], *probabilities[t])\n",
    "        \n",
    "        for t, v in region.items():\n",
    "            res[chunk] += v.expected_hotspots\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute expected hotspot propensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected hotspot propensity is the sum of expected hotspots across chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ttype = 'COADREAD'\n",
    "binsize = '1000kb'\n",
    "N = 100  \n",
    "n_muts = 300\n",
    "\n",
    "res = expected_hotspots_per_chunk(ttype, n_muts, N, binsize)\n",
    "expected_hotspot_propensity = res.values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hotspots_framework]",
   "language": "python",
   "name": "conda-env-hotspots_framework-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
